{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "The number of features in a real-life project can reach thousands even millions which can make training extremely slow. To mitigate that problem \n",
    "we can reduce the number of features which is called _dimensionality reduction_(for example in our mnist image dataset each pixel of the 28x28 image is a\n",
    "feature and we have observed that the white pixels are not important so we can eliminate them).  \n",
    "__Important Note__: We have to recon that reducing the number of features lead to lost of informations in most cases.  \n",
    "Reducing the dimensions of a dataset is also helpful for data visualization which is essential to communicate with non developper.\n",
    "In this chapter we will present the most common way to reduce dimensions of datasets.\n",
    "\n",
    "## Curse of Dimensionality\n",
    "\n",
    "As the number of features in a dataset increases, the amount of data required to maintain the same level of statistical significance grows exponentially.\n",
    "This phenomenon is known as the __curse of dimensionality__ and it can lead to overfitting, poor model performance, and increased computational complexity.\n",
    "Many methods have beed developped to mitigate this problem we will discuss them in the next sections.\n",
    "\n",
    "## Projection\n",
    "\n",
    "Instances in a datasets are often not uniformly spread out across all dimensions, actually most of them will lie in a subdimensional space relatively to\n",
    "the dataset. In simpler words most instances that are on a 3 dimensional dataset will be close to a plane instead of spreading accross all 3 dimensions.\n",
    "The concept is to simply take those instances and transform their coordinates into the subdimension that they are fitted to. \n",
    "\n",
    "## Manifold\n",
    "\n",
    "In many cases it might not be possible to project the instances in the dataset in a lower dimension because it is rolling into itself like in the\n",
    "_swiss roll toy_ dataset. Projecting the instances in 2d would squash them into each other resulting in a great lost of informations. The solution is\n",
    "to unroll the instances in a plane instead. The main motivation for manifold learning is that many high-dimensional datasets have an intrinsic \n",
    "low-dimensional structure that is not readily apparent in the original high-dimensional representation. By identifying and preserving this low-dimensional \n",
    "manifold structure, manifold learning techniques can provide a more meaningful and efficient representation of the data. Now that we know the most commonly\n",
    "used method to deal with high dimensional datasets, we are going to defined and implement the algorithms that are used as solution.\n",
    "\n",
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "The approach of this algorithm is simple. It works by first centering the data by subtracting the average value from each feature. Then, PCA finds new axes (called __principal components__) that line up with the directions where the data has the most variation. The principal components are arranged in order, \n",
    "from the one that captures the most variation to the one that captures the least. The original high-dimensional data is then projected onto the top few \n",
    "principal components, effectively reducing the number of dimensions. The key benefits of PCA are that it removes redundant or less important features, \n",
    "making the data easier to work with; it preserves the most essential information from the original high-dimensional data; the principal components are \n",
    "uncorrelated, which can be useful for further analysis; and it enables data visualization by reducing the dimensions to 2 or 3. To find the principal\n",
    "components we can use an equation called __SVD__(Singular Value Decomposition) we decompose the matrix of all the features into the matrix multiplication\n",
    "of 3 matrices ($\\mathsf{U}, \\Sigma, \\mathsf{V}$) where $\\mathsf{V}$ is the vector containing the features that are the principal components. Note that\n",
    "numpy possess an _svd()_ function. We can now project our dataset into a hyperplane defined by the dimensions of the principal components found, to do that\n",
    "we multiply the matrix of the dataset with the matrix of the principal components:\n",
    "$$X_{d-proj} = XW_d$$\n",
    "Here is a code implementation using scikit-learn's PCA class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(200, 1) - 0.5\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important problem will be to choose the right number of dimensions to converse the maximum accuracy for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mnist = fetch_openml('mnist-784', as_frame=False)\n",
    "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1 # This computes the number of dimensions necessary to conserve a 95% variance in our training set.\n",
    "# Another option is to set it directly during the PCA call by setting the n_components parameter between 0 and 1 (representing the ratio of variance we want to conserve)\n",
    "pca1 = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with this implementation is that we need the dataset to be able to fit in memory but to solve that scikit-learn offers us the _IncrementalPCA_\n",
    "class that allows us to be able to split the data into mini-batch first enabling online learning by the same occasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_train)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very high dimensional datasets PCA might be too slow, in this case we need to revert to random projection.\n",
    "\n",
    "## Random Projection\n",
    "\n",
    "The concept is to project the data into a lower dimensional space using a random linear projection. To know the number of dimensions we need to conserve we\n",
    "have an equation that determine that minimum number of dimensions if we want to preserve a tolerance of at least 10%. This equation is implemented in \n",
    "scikit-learn by the _johnson\\_lindenstrauss\\_min\\_dim()_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "\n",
    "m, e = 5_000, 0.1\n",
    "\n",
    "d = johnson_lindenstrauss_min_dim(m=m, eps=e)\n",
    "\n",
    "# Now we generate a matrix of shape [d, n] and use it to project the dataset from n dimensions to d\n",
    "n= 20_000\n",
    "np.random.seed(42)\n",
    "P = np.random.randn(d, n) / np.sqrt(d)\n",
    "\n",
    "X = np.random.randn(m, n) # Here we are just generating a random dataset\n",
    "\n",
    "X_reduces = X @ P.T\n",
    "# Alternatively we can use the GaussianRandomProjection class from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Linear Embedding(LLE)\n",
    "\n",
    "LLE is a nonlinear dimensionality reduction and a manifold technique that doesn't rely on projection or PCA. It measures the distance between a point and\n",
    "its nearest neighbors and search a for a lower dimensional representation that best conserve this distances. This method is very efficient to unroll\n",
    "manifold datasets such as the __swiss roll__ which we are going to use as an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42) # t is the variable containing the position of each instances along the rolled axis\n",
    "# It can be used as a label set for regression tasks purposes.\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_unrolled = lle.fit_transform(X_swiss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
