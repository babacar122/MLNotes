{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Neural Network for Computer Vision\n",
    "\n",
    "Convolutional neural networks are a type of architecture used for computer vision, voice recognition and natural language processing. It is mostly inspired\n",
    "from the functioning of the human visual cortex. Many neurons in the visual cortex actually have a small receptive field and the receptive fields of\n",
    "different neurons may overlap and together they form the entire visual field.\n",
    "\n",
    "## The convolutional layer\n",
    "The convolutional layer is the most important building block of a CNN. It applies a convolutional operation to the input image using a kernel (or filter) of learned weights to produce a feature map. The kernel slides over the input image(we can decide the size of each step using the _stride_ value), \n",
    "performing element-wise multiplications and summing the results to produce a single value in the output feature map. This operation is repeated across the \n",
    "entire image, allowing the network to detect various features such as edges, textures, and patterns. The convolutional layer helps in reducing the spatial \n",
    "dimensions of the input image while preserving the important features, making it easier for the network to learn and recognize complex patterns. An example \n",
    "is shown below:\n",
    "\n",
    "<div style=\"text-align: center;\"><img src=\"./images/cnn_convolution.png\" alt=\"Alt text\" title=\"Optional title\"></div>\n",
    "\n",
    "Below is an example code of a convolutional operation performed on random sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[9.37514827e-02 0.00000000e+00 3.43711913e-01 ... 2.73137093e-01\n",
      "   1.10400639e-01 0.00000000e+00]\n",
      "  [5.35154343e-02 0.00000000e+00 3.29980612e-01 ... 2.70694941e-01\n",
      "   1.09588221e-01 0.00000000e+00]\n",
      "  [4.38829958e-02 0.00000000e+00 3.76105368e-01 ... 2.72011727e-01\n",
      "   1.63754240e-01 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 3.50253373e-01 ... 1.81570113e-01\n",
      "   2.82854229e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.42238486e-01 ... 1.68399453e-01\n",
      "   2.92936474e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.40601146e-01 ... 1.62550807e-01\n",
      "   2.86762983e-01 0.00000000e+00]]\n",
      "\n",
      " [[1.14573747e-01 0.00000000e+00 3.39666873e-01 ... 2.93983907e-01\n",
      "   1.10954784e-01 0.00000000e+00]\n",
      "  [8.79369229e-02 0.00000000e+00 3.24241668e-01 ... 2.72835612e-01\n",
      "   1.13533206e-01 0.00000000e+00]\n",
      "  [3.95478383e-02 0.00000000e+00 3.33753496e-01 ... 2.51007289e-01\n",
      "   1.22187205e-01 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 3.69481862e-01 ... 1.68428242e-01\n",
      "   3.01864862e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.67167145e-01 ... 1.61490053e-01\n",
      "   3.01534355e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.58255029e-01 ... 1.58640504e-01\n",
      "   3.07360560e-01 0.00000000e+00]]\n",
      "\n",
      " [[8.24210271e-02 0.00000000e+00 3.16817939e-01 ... 2.77795851e-01\n",
      "   1.13267250e-01 0.00000000e+00]\n",
      "  [1.11198820e-01 0.00000000e+00 3.30532730e-01 ... 2.91983694e-01\n",
      "   1.10490821e-01 0.00000000e+00]\n",
      "  [7.49290064e-02 0.00000000e+00 3.15840632e-01 ... 2.83338279e-01\n",
      "   9.97671634e-02 0.00000000e+00]\n",
      "  ...\n",
      "  [3.90803441e-03 0.00000000e+00 3.47143888e-01 ... 1.90838128e-01\n",
      "   2.72208363e-01 0.00000000e+00]\n",
      "  [2.74881744e-03 0.00000000e+00 3.56796473e-01 ... 1.93439901e-01\n",
      "   2.85089970e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.51973265e-01 ... 1.75524339e-01\n",
      "   2.96721041e-01 0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[8.00462067e-02 0.00000000e+00 4.13085192e-01 ... 2.40928546e-01\n",
      "   1.64940596e-01 3.21705197e-03]\n",
      "  [7.95802549e-02 0.00000000e+00 3.95442694e-01 ... 2.41826639e-01\n",
      "   1.39882684e-01 1.64154768e-02]\n",
      "  [9.86355990e-02 0.00000000e+00 4.11901474e-01 ... 2.52232164e-01\n",
      "   1.13057218e-01 1.72789861e-02]\n",
      "  ...\n",
      "  [4.84995693e-02 0.00000000e+00 3.45542580e-01 ... 2.35586569e-01\n",
      "   1.16080351e-01 0.00000000e+00]\n",
      "  [1.11763731e-01 0.00000000e+00 3.17469180e-01 ... 2.25888059e-01\n",
      "   1.31694317e-01 0.00000000e+00]\n",
      "  [4.58465405e-02 0.00000000e+00 3.15282226e-01 ... 2.39708766e-01\n",
      "   1.59776136e-01 0.00000000e+00]]\n",
      "\n",
      " [[9.28092003e-02 0.00000000e+00 4.10121322e-01 ... 2.37887546e-01\n",
      "   1.48164943e-01 1.92024317e-02]\n",
      "  [8.18253085e-02 0.00000000e+00 3.95326585e-01 ... 2.40250781e-01\n",
      "   1.28436536e-01 3.09634628e-03]\n",
      "  [7.75200278e-02 0.00000000e+00 3.88300806e-01 ... 2.46343449e-01\n",
      "   1.11651286e-01 5.92507515e-03]\n",
      "  ...\n",
      "  [2.39779707e-02 0.00000000e+00 3.43924701e-01 ... 2.56176978e-01\n",
      "   1.58753514e-01 0.00000000e+00]\n",
      "  [8.57893005e-02 0.00000000e+00 3.48193824e-01 ... 2.34722480e-01\n",
      "   1.13152184e-01 0.00000000e+00]\n",
      "  [5.71425110e-02 0.00000000e+00 3.19121420e-01 ... 2.12724492e-01\n",
      "   1.69044018e-01 0.00000000e+00]]\n",
      "\n",
      " [[5.39444908e-02 0.00000000e+00 3.99427593e-01 ... 2.41952494e-01\n",
      "   1.67514011e-01 0.00000000e+00]\n",
      "  [4.67113368e-02 0.00000000e+00 3.93306404e-01 ... 2.43792921e-01\n",
      "   1.37808576e-01 1.23316509e-04]\n",
      "  [5.72516993e-02 0.00000000e+00 3.92339051e-01 ... 2.35851675e-01\n",
      "   1.24492615e-01 3.70687875e-03]\n",
      "  ...\n",
      "  [8.00827593e-02 0.00000000e+00 3.44644457e-01 ... 2.67239839e-01\n",
      "   1.91740438e-01 0.00000000e+00]\n",
      "  [5.93458861e-02 0.00000000e+00 3.90281707e-01 ... 2.75514603e-01\n",
      "   1.45665213e-01 0.00000000e+00]\n",
      "  [9.40505564e-02 0.00000000e+00 3.37567836e-01 ... 2.32324407e-01\n",
      "   1.30661890e-01 0.00000000e+00]]], shape=(64, 114, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)\n",
    "\n",
    "# Now we can create a convolutional layer and feed it our images\n",
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, activation=\"relu\") # if we do not specify an activation function the model would not be able to recognize complex patterns\n",
    "fmaps = conv_layer(images)\n",
    "\n",
    "print(fmaps[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "**Padding** is a technique used in Convolutional Neural Networks (CNNs) to control the spatial dimensions (width and height) of the output feature maps. It involves adding extra pixels around the border of an input image or feature map before applying a convolution operation. Padding is essential for several reasons:\n",
    "\n",
    "1. **Preserve Spatial Dimensions**: Without padding, the spatial dimensions of the output feature maps would decrease after each convolution operation. By adding padding, we can preserve the original dimensions, which is crucial for deep networks where multiple convolutional layers are stacked.\n",
    "\n",
    "2. **Maintain Information at Borders**: Padding ensures that the information at the borders of the input image is not lost since the operations are more \n",
    "often done in the centre of the image than in the borders. Without padding, the convolution operation would only consider the central part of the image, \n",
    "ignoring the borders.\n",
    "\n",
    "3. **Control Output Size**: Padding allows us to control the size of the output feature maps. By adjusting the amount of padding, we can ensure that the output dimensions match the desired size.\n",
    "\n",
    "There are different types of padding techniques:\n",
    "\n",
    "- **Valid Padding**: No padding is added, and the output feature map is smaller than the input.\n",
    "- **Same Padding**: Padding is added to ensure that the output feature map has the same spatial dimensions as the input.\n",
    "- **Full Padding**: Padding is added to ensure that the output feature map is larger than the input.\n",
    "\n",
    "In TensorFlow/Keras, padding can be specified using the `padding` parameter in the `Conv2D` layer. For example, `padding='same'` ensures that the output feature map has the same dimensions as the input. Here is a visual example:\n",
    "\n",
    "<div style=\"text-align: center;\"><img src=\"./images/cnn_padding.png\" alt=\"Alt text\" title=\"Optional title\"></div>\n",
    "\n",
    "Here is how you add padding in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, activation=\"relu\", kernel_size=3, padding=\"same\")\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "The second important components of CNNs are __pooling layers__. Their goal is to _subsample_(or shrink) the original input image in order to reduce the\n",
    "computational load and memory usage of the model. Other than that they also introduca a level of invariance to small translations. The most common type of \n",
    "pooling, where each region in the input is divided into small, non-overlapping sub-regions. The maximum value in each sub-region is selected to form the \n",
    "pooled feature map. This operation helps preserve the most prominent features. We also have average pooling instead of taking the maximum value, average \n",
    "pooling calculates the average value of each sub-region. This pooling method is less aggressive than max pooling and can smooth the feature map. Here is how\n",
    "to create a max pooling layer with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.10094817 0.07772617 0.13518608 ... 0.15399255 0.06089488 0.02264672]\n",
      "  [0.11356486 0.08491546 0.1431134  ... 0.16786408 0.0676508  0.        ]\n",
      "  [0.11287459 0.11104146 0.17762901 ... 0.17322905 0.06855411 0.        ]\n",
      "  ...\n",
      "  [0.10049663 0.04511965 0.2177978  ... 0.0828623  0.06301879 0.        ]\n",
      "  [0.08970017 0.05842229 0.22376192 ... 0.09498172 0.05991599 0.        ]\n",
      "  [0.06293146 0.29166228 0.2128299  ... 0.24305214 0.05796501 0.        ]]\n",
      "\n",
      " [[0.14413948 0.07996679 0.13051876 ... 0.16011535 0.         0.        ]\n",
      "  [0.15504767 0.0839283  0.13591729 ... 0.17148659 0.         0.        ]\n",
      "  [0.15185237 0.06822395 0.12205692 ... 0.16925704 0.         0.        ]\n",
      "  ...\n",
      "  [0.0442225  0.09662327 0.22528875 ... 0.09666712 0.         0.        ]\n",
      "  [0.05753833 0.07644734 0.224721   ... 0.08089541 0.         0.        ]\n",
      "  [0.0727421  0.2599259  0.21103056 ... 0.21607596 0.02872087 0.        ]]\n",
      "\n",
      " [[0.1716296  0.05790159 0.12007    ... 0.1559475  0.         0.        ]\n",
      "  [0.17004061 0.05788299 0.12031252 ... 0.15805994 0.         0.        ]\n",
      "  [0.18001524 0.05736471 0.12210055 ... 0.15994976 0.         0.        ]\n",
      "  ...\n",
      "  [0.04178871 0.12549749 0.21296212 ... 0.11430053 0.         0.        ]\n",
      "  [0.01281912 0.11541077 0.2143421  ... 0.11011922 0.         0.        ]\n",
      "  [0.         0.2980548  0.21907401 ... 0.20703675 0.02815008 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.11916986 0.0747906  0.18347435 ... 0.13974407 0.         0.        ]\n",
      "  [0.12544762 0.07559279 0.18153441 ... 0.13156618 0.         0.        ]\n",
      "  [0.13125893 0.06459514 0.14779116 ... 0.13829671 0.         0.        ]\n",
      "  ...\n",
      "  [0.17814891 0.06244881 0.15667608 ... 0.12689231 0.         0.        ]\n",
      "  [0.15127754 0.06330434 0.1427109  ... 0.1554072  0.         0.        ]\n",
      "  [0.15006413 0.24396211 0.13562953 ... 0.29532132 0.         0.        ]]\n",
      "\n",
      " [[0.10169294 0.06868862 0.18734424 ... 0.14304736 0.         0.        ]\n",
      "  [0.11143545 0.0798227  0.17913213 ... 0.12057011 0.         0.        ]\n",
      "  [0.13344714 0.06957893 0.16414878 ... 0.12905122 0.         0.        ]\n",
      "  ...\n",
      "  [0.13642307 0.12586327 0.14490667 ... 0.17982268 0.         0.        ]\n",
      "  [0.1544667  0.06214107 0.15574868 ... 0.1443771  0.         0.        ]\n",
      "  [0.11730037 0.26957437 0.14548188 ... 0.28493428 0.         0.        ]]\n",
      "\n",
      " [[0.07945135 0.08241305 0.21282375 ... 0.18292597 0.         0.        ]\n",
      "  [0.09671915 0.07658759 0.1734967  ... 0.1876206  0.         0.        ]\n",
      "  [0.11772621 0.06120516 0.17050731 ... 0.18027854 0.         0.        ]\n",
      "  ...\n",
      "  [0.14765328 0.12194857 0.1943852  ... 0.197249   0.         0.        ]\n",
      "  [0.14829619 0.06521919 0.17798743 ... 0.16522235 0.00772404 0.        ]\n",
      "  [0.10977309 0.26691586 0.14793207 ... 0.2820679  0.00930801 0.04426269]]], shape=(35, 60, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2) # To use average pooling we can use AveragePooling2D\n",
    "pooled_images = max_pool(fmaps)\n",
    "print(pooled_images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building entire CNN architectures. A CNN architecture is mostly composed of a few convolutional layers(generally followed by a RELU\n",
    "activation layer), a pooling layer and other convolutional layers so on and so forth... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Notes\\handsonml\\mlnotes\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]), # We typically increase the number of filter in the model as the images progress through the layer in order for it to learn progressively deeper and deeper patterns\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a ResNet CNN with keras\n",
    "\n",
    "ResNet(__Residual Network__) is a type of deep neural network architecture that addresses the problem of vanishing gradients in very deep networks, \n",
    "allowing for the training of much deeper networks than was previously feasible. The core idea of ResNet is to learn the \"residual\" mapping instead of \n",
    "trying to learn the direct mapping. If the desired underlying mapping is H(ð‘¥), ResNet reformulates it as F(x) + x where F(x) = H(x) - x, it is the reisdual\n",
    "mapping that needs to be learnt. ResNet introduces shortcut or _skip connections_, which are the connections that skip one or more layers. So let's\n",
    "implement a __ResNet-34__(34 layers) with keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and object localization\n",
    "\n",
    "Localizing an object in a picture can be expressed as a regression task, to predict a bounding box around the object, a common approach is to predict the \n",
    "horizontal and vertical coordinates of the objectâ€™s center, as well as its height and width. It does not require much change to the model; we just\n",
    "need to add a second dense output layer with four units(on top of the average pooling layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"], loss_weights=[0.8, 0.2], optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we have now is that the data does not have bounding box around the flowers in the images, to solve this issue we can use software like \n",
    "_ImgLab_, _VGGImage_ or use crowdsourcing online(paying freelances to label the data for us). Note that the bounding boxes values that you will get\n",
    "should be normalized like any other features. The most common loss function used for this type of tasks is the _intersection over union loss function_ and\n",
    "it is defined in _tf.keras.metrics.MeanIoU_.  \n",
    "If the image contains multiple objects "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
