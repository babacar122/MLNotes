{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "In reinforcement learning an _agent_ make _observations_ and takes _actions_ in an _environment_ and receive _rewards_(or negative rewards) from the \n",
    "environment. The goal of the agent to act in the best way to receive the maximum reward. The algorithm used by the agent to determine its action is called\n",
    "__policy__. A policy can be a neural network that takes the observations as inputs and outputs the action to take. But the policy sometime will not take\n",
    "observations like in the case of a vacuum cleaner whose rewards is determined by the amount of dust it picks up. The policy could make the robot move \n",
    "forward with some probability _p_ and turn randomly left or right with a probability _1 - p_. The random angle's value would oscilate from -r to r. Since\n",
    "the policy involves randomness it is called _schotastic policy_. To find the best set of hyperparameters we have multiple methods at our disposition such as\n",
    "__policy search__ which is simply trying out many different sets of values and keeping the one with the best performances. But when the __policy space__ is\n",
    "too large this will not lead to a good result. Instead we would use __genetic algorithms__ which is generating 100 policies and keeping only the 20 best,\n",
    "generating variants of those left and iterating until we find an appropriate final policy. We also can use optimization techniques, by evaluating the \n",
    "gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards. This \n",
    "approach is called __policy gradients (PG)__.\n",
    "\n",
    "## Using OpenAI gym\n",
    "\n",
    "In order to train our agents we will need a simulated environment. We are going to use OpenAI gym librairy which gives us many different simulated\n",
    "environment to train an agent to be able to play an atari game autonomously. We are going to create a CartPole environment. This is a 2D simulation in \n",
    "which a cart can be accelerated left or right in order to balance a pole placed on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the environment we need to initialize it. It will return the first observation. Observations depends on the type of environment, in our case\n",
    "it is in the form of 1D numpy array containing 4 floats containing the cart's horizontal position, its velocity, the angle of the pole and its angular\n",
    "velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Observation: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Truncated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# We can also look at the action space\n",
    "print(env.action_space)\n",
    "\n",
    "# Let's do an action and look at the result\n",
    "action = 1 # Leaning the pole toward the right\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {done}\")\n",
    "print(f\"Truncated: {truncated}\")\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are coding to code a simple policy  that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward \n",
    "the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.698 8.389445512070509 24.0 63.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(np.mean(totals), np.std(totals), min(totals), max(totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the resuls of it are not very good. Alternatively we can try to make a neural network policy. It will output a probability for each action\n",
    "and one action will be chosen randomly according to the weight of its probability. In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier the best way to evaluate this types of algorithms is __policy gradients__. One of the most popular one is called reinforce algorithms\n",
    "and it is defined as follows:\n",
    "- First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more \n",
    " likely—but don’t apply these gradients yet.\n",
    "- Once you have run several episodes, compute each action’s advantage.\n",
    "- If an action’s advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action\n",
    " even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and you want to apply \n",
    " the opposite gradients to make this action slightly less likely in the future. The solution is to multiply each gradient vector by the corresponding \n",
    " action’s advantage.\n",
    "- Finally, compute the mean of all the resulting gradient vectors, and use it to perform a gradient descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # type: ignore\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads\n",
    "\n",
    "# Now let’s create another function that will rely on the play_one_step() function to play multiple episodes, returning all the rewards and gradients for each episode and each step:\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, rewards, done, truncated, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(rewards)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will use the play_multiple_episodes() function to play the game several times (e.g., 10 times), then it will go back and look at all the \n",
    "rewards, discount them, and normalize them. To do that, we need a couple more functions; the first will compute the sum of future discounted rewards at \n",
    "each step, and the second will normalize all these discounted rewards (i.e., the returns) across many episodes by subtracting the mean and dividing by the \n",
    "standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(rewards - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    rewards_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / rewards_std for discounted_rewards in all_discounted_rewards]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define the hyperparameters. We will run 150 training iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. \n",
    "We will use a discount factor of 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "# We also need an optimizer and a loss function\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "# Then we can run the training loop\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index] for episode_index, final_rewards in enumerate(all_final_rewards) for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple policy gradients algorithm we just trained solved the CartPole task, but it would not scale well to larger and more complex tasks\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "The mathematician __Andrey Markov__ studied schotastic processes with no memory, he called it the __Markov chains__. A Markov Decision Process (MDP) is a core concept in reinforcement learning that defines how an agent interacts with its environment to make sequential decisions. It is characterized by five key components: states, which describe the different situations the agent can encounter; actions, which are the choices available to the agent in each state; a transition function that determines the probability of moving from one state to another given a specific action; a reward function, which provides feedback (positive or negative) based on the outcome of an action; and a discount factor, which balances the value of immediate rewards against future rewards. The agent's objective is to learn a policy. MDPs assume the Markov property, meaning that the future state depends only on the current state and action, not on past states, simplifying the decision-making process. By optimizing the policy, the agent learns to achieve the highest cumulative reward over time, even in uncertain or stochastic environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
