{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders and diffusion models\n",
    "\n",
    "Autoencoders are a type of artificial neural network used for unsupervised learning tasks, primarily designed for data compression and feature extraction or\n",
    "dimensionality reduction. They work by learning a compressed representation (encoding) of input data, which is then reconstructed (decoded) to resemble the \n",
    "original input. They can learn from dense representation of the input data (called __latent representations__ or __codings__). These type of models can \n",
    "also serve as generative models (which mean creating new data that looks similar to the fed input). GANs (__Generative Adverserial Networks__) are also a \n",
    "type of model capable of generating data; they can also be used for __super resolution__ which is increasing the resolution of an image, image editing, \n",
    "video generations and many other things. Recently diffusion models were also introduces for data generation but they are much slower than the other ones.  \n",
    "For autoencoders they simply learn to copy their inputs into their outputs for data generation but we can limit the size of the data so that they don't \n",
    "simply do a direct copy of the inputs.  \n",
    "For GANs, they are composed of 2 neural networks, one for data generation and one that try to tell if the generated data is fake or not. In this chapter we will start by exploring in more depth how autoencoders work and how to use them for dimensionality reduction, feature extraction, unsupervised pretraining, \n",
    "or as generative models. \n",
    "\n",
    "## Latent representation\n",
    "\n",
    "The best way to make model like autoencoders learn patterns is to constraints their latent representations to avoid them just memorizing the inputs and\n",
    "regurgitating them onto us. For that an autoencoder is always composed of an encoder(a recognition network) and a decoder(the generator) that convert the\n",
    "internal representations into outputs. It has the roughly the same architecture as a MultiLayer Perceptron but it have to possess the same number of \n",
    "output neurons as number of inputs(the outputs are ofter called __reconstructions__). The cost function contains a reconstruction loss that penalizes the\n",
    "model when the reconstructions are different from the inputs. Since the codings have a lower dimension than the inputs, we say that the autoencoder is\n",
    "__undercomplete__. \n",
    "\n",
    "## Using PCA on an autoencoder for dimensionality reduction\n",
    "\n",
    "If an autoencoder uses only linear activation functions and Mean Square Error as a loss function then it perform Principal Component Analysis. The following\n",
    "code does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "[[ 0.16522342  0.32384145]\n",
      " [-0.23413214 -0.5091713 ]\n",
      " [ 0.40434515 -0.20902419]\n",
      " [ 0.06688297  0.27796385]\n",
      " [-0.14037907  0.17638358]\n",
      " [ 0.35200047  0.10714732]\n",
      " [ 0.0584873   0.32540306]\n",
      " [-0.36659646 -0.4886512 ]\n",
      " [ 0.10589898 -0.12928466]\n",
      " [ 0.04103708 -0.7495854 ]\n",
      " [-0.1117608  -0.59058994]\n",
      " [-0.6049117  -0.32423735]\n",
      " [-0.19678414 -0.3413568 ]\n",
      " [ 0.21883571  0.25637305]\n",
      " [ 0.3481742  -0.07300492]\n",
      " [-0.16636962 -0.73139524]\n",
      " [-0.06996274 -0.470938  ]\n",
      " [-0.11682546  0.24838498]\n",
      " [ 0.14649338  0.4748063 ]\n",
      " [ 0.21879315  0.08633684]\n",
      " [-0.27510154  0.0593375 ]\n",
      " [-0.37081712 -0.89605814]\n",
      " [ 0.33813936 -0.11352368]\n",
      " [ 0.28720796 -0.22949375]\n",
      " [ 0.38569105 -0.3845369 ]\n",
      " [-0.13480824 -0.13314119]\n",
      " [-0.5750909  -0.09549962]\n",
      " [ 0.31575763 -0.539759  ]\n",
      " [-0.5180564  -0.34529066]\n",
      " [-0.09816223 -0.10286307]\n",
      " [-0.10133952 -0.5949105 ]\n",
      " [-0.39297435  0.00253003]\n",
      " [ 0.38198626 -0.21721609]\n",
      " [-0.2501844  -0.6972898 ]\n",
      " [-0.19265285  0.04769717]\n",
      " [ 0.14221668  0.01176004]\n",
      " [-0.71881443 -0.34221166]\n",
      " [ 0.07161689 -0.01432   ]\n",
      " [-0.4573593  -0.19042474]\n",
      " [-0.29921275 -0.56334895]\n",
      " [-0.24184307 -0.89058274]\n",
      " [ 0.68335235 -0.01077953]\n",
      " [ 0.64786017  0.10781439]\n",
      " [ 0.01942098  0.3504951 ]\n",
      " [-0.56220853 -0.24450655]\n",
      " [ 0.09599131  0.30761525]\n",
      " [-0.1578154  -0.8144846 ]\n",
      " [-0.13274562 -0.4133222 ]\n",
      " [ 0.04057014  0.25347948]\n",
      " [ 0.23647422  0.26243815]\n",
      " [ 0.13707101  0.20402762]\n",
      " [ 0.6620673   0.07219459]\n",
      " [-0.04858822 -0.82517964]\n",
      " [-0.07734895 -0.65100384]\n",
      " [-0.1311211  -0.71810544]\n",
      " [ 0.1315499   0.29544732]\n",
      " [-0.45938402 -0.1374944 ]\n",
      " [ 0.1732797   0.15381035]\n",
      " [ 0.29540157 -0.20921636]\n",
      " [-0.4419744  -0.5108942 ]\n",
      " [-0.09064037 -0.54412305]\n",
      " [ 0.22930586  0.38875112]\n",
      " [-0.5888053  -0.5160414 ]\n",
      " [-0.36427838 -0.73948944]\n",
      " [-0.09255779 -0.6916328 ]\n",
      " [ 0.21386999  0.5918534 ]\n",
      " [-0.14269024 -0.35432377]\n",
      " [-0.18763086 -0.00561311]\n",
      " [-0.20737875 -0.00151691]\n",
      " [ 0.08828449  0.2808136 ]\n",
      " [-0.4473893  -0.32419598]\n",
      " [-0.0142585  -0.71004987]\n",
      " [ 0.23098809  0.3518862 ]\n",
      " [ 0.17651272 -0.76404345]\n",
      " [-0.47160858 -0.29296583]\n",
      " [-0.14668375 -0.57603955]\n",
      " [-0.28998655 -0.76641494]\n",
      " [ 0.3837905  -0.1997985 ]\n",
      " [-0.02035934  0.02943378]\n",
      " [-0.35843053 -0.04692105]\n",
      " [ 0.15298909  0.14463307]\n",
      " [-0.15937924  0.2792244 ]\n",
      " [-0.16814458 -0.71329564]\n",
      " [-0.07649612 -0.1465359 ]\n",
      " [ 0.35075116 -0.534634  ]\n",
      " [ 0.50413084 -0.20081024]\n",
      " [-0.3196935  -0.39416188]\n",
      " [-0.40433368 -0.5439108 ]\n",
      " [-0.5056529   0.07271836]\n",
      " [-0.5426394  -0.22169258]\n",
      " [-0.23866239 -0.17854413]\n",
      " [ 0.24241424 -0.15616174]\n",
      " [-0.19975597 -0.42287415]\n",
      " [ 0.49103165  0.18793869]\n",
      " [ 0.14360529 -0.1390997 ]\n",
      " [-0.4018954   0.045051  ]\n",
      " [ 0.14714998  0.03938993]\n",
      " [-0.3658032  -0.17282063]\n",
      " [-0.03774619 -0.18163106]\n",
      " [-0.57062507 -0.16202247]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2) # we don't need any activation function\n",
    "])\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n",
    "# Let's train it on a 3 dimensional dataset and reduce it dimension in 2d\n",
    "num_samples = 100\n",
    "X = np.random.rand(num_samples, 3)\n",
    "\n",
    "history = autoencoder.fit(X, X, epochs=500, verbose=False)\n",
    "codings = encoder.predict(X)\n",
    "\n",
    "\n",
    "print(codings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked autoencoders\n",
    "\n",
    "Autoencoder like any other model can have multiple layers(in that case they are called __stacked autoencoders__). Adding more layers make them prone to\n",
    "learn more complex codings but we have to be careful of it being too powerful then overfitting might take place. Such network is usually symmetrical around\n",
    "the central hidden layer. Here is an implementation of a stacked autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 3 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_5_1/sequential_4_1/reshape_1/Reshape)' with input shapes: [?,3], [?,28,28].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m stacked_ae \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([stacked_encoder, stacked_decoder])\n\u001b[0;32m     13\u001b[0m stacked_ae\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mstacked_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bn489\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bn489\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1158\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1156\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1157\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 3 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_5_1/sequential_4_1/reshape_1/Reshape)' with input shapes: [?,3], [?,28,28]."
     ]
    }
   ],
   "source": [
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "])\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = stacked_ae.fit(X, X, epochs=20, validation_data=(X[80:], X[80:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an autoencoder is symmetrical one way to speed up training and reduce overfitting is to tie the weights of the decoder to the weights of the encoder\n",
    "effectively halving the number of weights in the model. To do that we can create a custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\", shape=self.dense.input_shape[-1], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(Z + self.biases)\n",
    "    \n",
    "# Now we can build an autoencoder with the weights of the decoder tied to the weights of the encoder\n",
    "dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "dense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "\n",
    "tied_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = tf.keras.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"relu\"),\n",
    "    DenseTranspose(dense_1),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoders\n",
    "\n",
    "If we want to use autoencoders for images we can use convolutional autoencoder. Like a regular CNN the encoder is composed of convolutional layers and \n",
    "pooling layers. It oftens reduces the dimensions of the input(the width and the height of the images) but increase the number of feature maps. The decoder\n",
    "should do the reverse(upscale the image and reduce its depth). For that we can use transpose convolutional layers in the decoder or a combination of \n",
    "upsampling layers with convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()\n",
    "])\n",
    "\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to force autoencoders to learn patterns is to add noise to images and ask it to denoise them. Here is an implementation of such autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "])\n",
    "\n",
    "dropout_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also constraint the model by adding an appropriate term to the cost function. The autoencoder will be pushed to reduce the number of active number\n",
    "in the coding layer(this often leads to good feature extractions). This method is called __sparse autoencoding__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_l1_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
    "])\n",
    "\n",
    "sparse_l1_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ActivityRegularization layer just returns its inputs, but as a side effect it adds a training loss equal to the sum of the absolute values of its \n",
    "inputs. It is equivalent to add the _activity\\_regularizer=tf.keras.regularizers.l1(1e-4)_ option to the previous layer. This will penalize the model if it\n",
    "doesn't output codings close to 0 but because it will also be penalized if it doesn't reconstruct the inputs correctly, it will also output a few non zero\n",
    "values.\n",
    "\n",
    "## Variational Autoencoders\n",
    "\n",
    "This is one of the most popular type of autoencoders is the variational autoencoders. They are quite different from the other autoencoders in the sense that\n",
    "they are probabilistic(meaning that their outputs are partly determined by chance) and they are __generative autoencoders__(meaning that they can generate \n",
    "new instances that look like they were sampled from the training set). Variational autoencoder performs __variational Bayesian inference__ which is an\n",
    "approximation of bayesian inference. Recall that Bayesian inference means updating a probability distribution based on new data, using equations derived \n",
    "from Bayes’ theorem. The original distribution is called the _prior_ while the updated one is called the _posterior_. Instead of directly producing the \n",
    "coding of a given input, it produces a mean coding $\\mathbf{\\mu}$ and a standard deviation $\\mathbf{\\sigma}$. And the actual coding is sampled randomly \n",
    "from a gaussian distribution of mean $\\mathbf{\\mu}$ and standard deviation $\\mathbf{\\sigma}$. The loss function is composed of 2 parts: the usual\n",
    "reconstruction loss that push the autoencoder to generate instances close to the inputs and the second is the latent loss that pushes the autoencoder to \n",
    "have codings that look as if they were sampled from a simple Gaussian distribution. The latent loss can be computed using the following equation:\n",
    "$$\\mathbf{L} = -\\frac{1}{2}\\sum_{i=1}^{n}[1 + log(\\sigma _i^2) - \\sigma _i^2 - \\mu _i^2] $$\n",
    "Where:\n",
    "- __n__ is the coding's dimensionality.\n",
    "- $\\mu _i$ is the mean of the coding of the $i^{th}$ instance in the dataset.\n",
    "- $\\sigma _i$ is the standard deviation of the $i^{th}$ instance.\n",
    "\n",
    "A common variation to this equation is to make the encoder output $\\gamma = log(\\sigma ^2)$ to speed up training:\n",
    "$$\\mathbf{L} = -\\frac{1}{2}\\sum _{i=1}^{n}(1 + \\gamma _i - \\exp(\\gamma _i) - \\mu _i^2)$$\n",
    "We are going to build a variational autoencoder with the fashion MNIST dataset but we first need a custom layer to sample a coding give the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the model using the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 10\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[28, 28])\n",
    "Z = tf.keras.layers.Flatten()(inputs)\n",
    "Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\n",
    "Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\n",
    "codings_mean = tf.keras.layers.Dense(codings_size)(Z)\n",
    "codings_log_var = tf.keras.layers.Dense(codings_size)(Z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = tf.keras.Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "# The decoder is a regular decoder\n",
    "decoder_input = tf.keras.layers.Input(shape=[codings_size])\n",
    "x = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_input)\n",
    "x = tf.keras.layers.Dense(150, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(28 * 28)(x)\n",
    "outputs = tf.keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder= tf.keras.Model(inputs=[decoder_input], outputs=[outputs])\n",
    "\n",
    "# Now the autoencoder\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# We need to add the latent and reconstruction loss\n",
    "latent_loss = -0.5 * tf.reduce_sum(1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean), axis=-1)\n",
    "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)\n",
    "\n",
    "variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128, validation_data=(X_valid, X_valid))\n",
    "\n",
    "# and now we can try generating fashion images\n",
    "codings = tf.random.normal(shape=[3 * 7, codings_size])\n",
    "images = variational_decoder(inputs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks(GANs)\n",
    "\n",
    "The concept of GANs is to train 2 neural network to compete against each other hoping that it will make them excel. The 2 model are a __generator__: it \n",
    "takes a random distribution as inputs and output some data, and a __discriminator__ that try to tell if the generated data is fake or not. Knowing that we\n",
    "cannot train a GAN like a regular neural network. First we train the discriminator by giving it as inputs a set of real inputs and an equal number of fake\n",
    "inputs generated from the generator(the labels of the real data is set to 1 and 0 for the fake ones). After that we start training the generator to generate\n",
    "other fake data while still using the discriminator to try to make distinction between the real data and the fake data(the weights of the discriminator are\n",
    "frozen during this step so that backpropagation only affect the generator). We are going to try to implement it on the fashion mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 30\n",
    "\n",
    "dense = tf.keras.layers.Dense\n",
    "generator = tf.keras.Sequential([\n",
    "    dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    dense(28*28, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "gan = tf.keras.Sequential([generator, discriminator])\n",
    "\n",
    "# Now we compile the GAN\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "batch_size= 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "# Since the training steps are quite unusual we need to build our own training loop\n",
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch in dataset:\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_imgs = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_imgs, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            gan.train_on_batch(noise, y2)\n",
    "\n",
    "\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can randomly sample a coding from a gaussian distribution and ask the generator to create new images from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_imgs = generator.predict(codings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training the GANs may reaches a state called __Nash equilibrium__ where both the generator and the discriminator wouldn't do better if they change \n",
    "strategy assuming the other one doesn't change his. Meaning when the generator produce perfectly realistic images and the discriminator is force to guess\n",
    "50% true and 50% fake. Reaching the equilibrium is the goal but the difficulty is called __mode collapse__. It is when the outputs of the generator become\n",
    "less diverse(suppose the generator becomes good at generating a certain type of images like shoes it will fool the discriminator which in turn will \n",
    "encourage the generator to generate more image of shoes), it will eventually become less viable at generating al the other types of images. A popular \n",
    "technique called __experience replay__ consists of storing the images produced by the generator at each iteration in a replay buffer (gradually dropping \n",
    "older generated images) and training the discriminator using real images plus fake images drawn from this buffer (rather than just fake images produced by \n",
    "the current generator). This reduces the chances that the discriminator will overfit the latest generator’s outputs. Another common technique is called __mini-batch discrimination__: it measures how similar images are across the batch and provides this statistic to the discriminator, so it can easily \n",
    "reject a whole batch of fake images that lack diversity. This encourages the generator to produce a greater variety of images, reducing the chance of mode \n",
    "collapse.\n",
    "\n",
    "## Deep convolutional GANs\n",
    "\n",
    "Researchers have come up for a way to build stable deep convolutional GANs:\n",
    "- Replace any pooling layers by strided convolutions in the discriminator and with transposed convolutions in the generator.\n",
    "- Use batch normalization in both the discriminator and the generator except in the output layer of the generator and the input layer of the discriminator.\n",
    "- Remove fully connected hidden layers\n",
    "- Use ReLU activation in all the layers in the generator except in the output layer which should use tanh.\n",
    "- Use LeakyReLU activation for all the layers in the discriminator.  \n",
    "\n",
    "Here is an example on the fashion MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 100\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranpose(64, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\", activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\", activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "deep_gan = tf.keras.Sequential([generator, discriminator])\n",
    "\n",
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would produce fairly good images though but to produce larger images researchers suggests producing small images at the beggining of the \n",
    "generator and upsample the image progressively by adding more layers at the end of the generator and at the beggining of the discriminator. This technique\n",
    "is called __Progressive GANs__.\n",
    "\n",
    "## Diffusion models\n",
    "\n",
    "Researchers found a way to generate very convincing images using diffusion models, their core method is called \n",
    "__denoising diffusion probabilistic model(DDPM)__. They are much easier to train than GANs and the images it outputs are more diverse. DDPM works as follows\n",
    ",suppose you have an image of a cat noted $\\mathbf{x_0}$ and at each time step _t_ we add a bit of gaussian noise to it with mean 0 and variance $\\beta _t$\n",
    "(this noise is called __isotropic__) we obtain the image $x_1$, $x_2$ ... until the cat is completely recovered by the noise(the last time step is noted T).\n",
    "This process is the _forward process_. The forward process is summarize in the following equation:\n",
    "$$q(\\mathbf{x}_t, \\mathbf{x}_{t-1}) = \\mathbf{N}(\\sqrt{1 - \\beta _t}x_{t-1}, \\beta _t \\mathbf{I}) $$\n",
    "\n",
    "Note that every pixel is rescaled at each time step by a value of $\\sqrt{1 - \\beta _t}$ to ensure that the mean of the pixel values gradually approaches 0, \n",
    "since the scaling factor is a bit smaller than 1. We can shorten this equation, it exists a way to compute an image $x_t$ given $x_0$ without having to\n",
    "compute $x_1$, $x_2$ ...etc:\n",
    "$$q(\\mathbf{x}_t, \\mathbf{x}_0) = \\mathbf{N}(\\sqrt{\\alpha _t}x_0, (1 - \\alpha _t)\\mathbf{I}) $$\n",
    "\n",
    "Now the goal of our model is to be able to reverse the process. The first thing we need to do is to code the forward process. For this, we will first need \n",
    "to implement the variance schedule. How can we control how fast the cat disappears? Initially, 100% of the variance comes from the original cat image. Then \n",
    "at each time step t, the variance gets multiplied by 1 – $\\beta _t$ , as explained earlier, and noise gets added. So, the part of the variance that comes \n",
    "from the initial distribution shrinks by a factor of 1 – $\\beta _t$ at each step. If we define $\\alpha$ = 1 – $\\beta$, then after t time steps, the cat\n",
    "signal will have been multiplied by a factor of $\\bar{\\alpha}_t$ = $\\alpha _t$. It’s this “cat signal” factor $\\bar{\\alpha}$ that we want to schedule so it \n",
    "shrinks down from 1 to 0 gradually between time steps 0 and T. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
    "    t = np.arange(T + 1)\n",
    "    f = np.cos((t / T + s) / (1 + s) * np.pi / 2)**2\n",
    "    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n",
    "    alpha = np.append(alpha, 1).astype(np.float32)\n",
    "    beta = 1 - alpha\n",
    "    alpha_cumprod = np.cumprod(alpha)\n",
    "    return alpha, alpha_cumprod, beta\n",
    "\n",
    "T = 4000\n",
    "alpha, alpha_cumprod, beta = variance_schedule(T)\n",
    "# This function will will take a batch of clean images from the dataset and prepare them\n",
    "def prepare_batch(X):\n",
    "    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1\n",
    "    X_shape = tf.shape(X)\n",
    "    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T+1, dtype=tf.int32)\n",
    "    alpha_cm = tf.gather(alpha_cumprod, t)\n",
    "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
    "    noise = tf.random.normal(X_shape)\n",
    "    return {\"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise, \"time\": t}, noise\n",
    "\n",
    "def prepare_dataset(X, batch_size=32, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=10_000)\n",
    "    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n",
    "\n",
    "class TimeEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, T, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(T + 1), 2 * np.arange(embed_size // 2))\n",
    "        t_emb = np.empty((T + 1, embed_size))\n",
    "        t_emb[:, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        t_emb[:, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.time_encodings = tf.constant(t_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(self.time_encodings, inputs)\n",
    "\n",
    "\n",
    "train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\n",
    "valid_set = prepare_dataset(X_valid, batch_size=32)\n",
    "\n",
    "# Now time to build the diffusion model\n",
    "def build_diffusion_model():\n",
    "    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n",
    "    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n",
    "    time_enc = TimeEncoding(T, embed_size)(time_input)\n",
    "\n",
    "    dim = 16\n",
    "    Z = tf.keras.layers.ZeroPadding2D((3, 3))(X_noisy)\n",
    "    Z = tf.keras.layers.Conv2D(dim, 3)(Z)\n",
    "    Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "    Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "\n",
    "    time = tf.keras.layers.Dense(dim)(time_enc)  # adapt time encoding\n",
    "    Z = time[:, tf.newaxis, tf.newaxis, :] + Z  # add time data to every pixel\n",
    "\n",
    "    skip = Z\n",
    "    cross_skips = []  # skip connections across the down & up parts of the UNet\n",
    "\n",
    "    for dim in (32, 64, 128):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        cross_skips.append(Z)\n",
    "        Z = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(Z)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, strides=2,\n",
    "                                           padding=\"same\")(skip)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        skip = Z\n",
    "\n",
    "    for dim in (64, 32, 16):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.UpSampling2D(2)(Z)\n",
    "\n",
    "        skip_link = tf.keras.layers.UpSampling2D(2)(skip)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, padding=\"same\")(skip_link)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        Z = tf.keras.layers.concatenate([Z, cross_skips.pop()], axis=-1)\n",
    "        skip = Z\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 3, padding=\"same\")(Z)[:, 2:-2, 2:-2]\n",
    "    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])\n",
    "\n",
    "# And finnaly a function to generate the images\n",
    "def generate(model, batch_size=32):\n",
    "    X = tf.random.normal([batch_size, 28, 28, 1])\n",
    "    for t in range(T, 0, -1):\n",
    "        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\n",
    "        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n",
    "        X = (1 / alpha[t] ** 0.5 * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise) + (1 - alpha[t]) ** 0.5 * noise)\n",
    "    return X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
